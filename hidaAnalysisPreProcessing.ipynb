{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from scipy import signal\n",
    "\n",
    "# Helper functions used in other parts of the code\n",
    "import HelperModules\n",
    "import EngineeredFeatures\n",
    "\n",
    "ds = xr.open_dataset('MyChallengePaleo/T2m_R1_ym_1stMill.nc')\n",
    "T2m_R1 = ds.to_dataframe()\n",
    "\n",
    "TSI = HelperModules.getForcingData('MyChallengePaleo/Solar_forcing_1st_mill.nc','TSI')\n",
    "AOD = HelperModules.getForcingData('MyChallengePaleo/Volc_Forc_AOD_1st_mill.nc','AOD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's pre-process the data using a scaling technique\n",
    "---------\n",
    "Namely, the \"RobustScaler\" from scikit-learn. Of course this does not add any information, but it may improve the performance of some subesquent tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "T2m_R1['T2m_transf'] = scaler.fit_transform(T2m_R1)\n",
    "#scaler.inverse_transform(scaler.transform(T2m_R1))\n",
    "T2m_R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "HelperModules.plotTimeSlice(T2m_R1['T2m'],1,fig,ax1)\n",
    "HelperModules.plotTimeSlice(T2m_R1['T2m_transf'],1,fig,ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we Cluster the data using k-Means, Agglomerative, or DBScan?\n",
    "----------\n",
    "Before going down the clustering path, it makes sense to ask: what dimensions make sense to consider?\n",
    " - Latitude and longitude for sure\n",
    " - Also time?\n",
    " - Does it make sense to engineer any features that encapsulate e.g. year-to-year changes?\n",
    " - Do we want to consider temperature, or temperature anomaly?\n",
    " - Do we want these clusters to be useful for some final goal?\n",
    "\n",
    "\n",
    "Why I am skeptical about clustering\n",
    "--------\n",
    "What will clustering achieve in this case? Probably it will cluster times and latitudes that are the same temperature. This means it might cluster a particularly cold year in the tropics with a bunch of data from the temperate zones. What does this tell us? Probably nothing.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
